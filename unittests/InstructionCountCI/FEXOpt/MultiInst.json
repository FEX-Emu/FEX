{
  "Features": {
    "Bitness": 64,
    "EnabledHostFeatures": [],
    "DisabledHostFeatures": [
      "SVE128",
      "SVE256",
      "RPRES",
      "AFP"
    ]
  },
  "Comment": [
    "These are instruction combinations that could be more optimal if FEX optimized for them"
  ],
  "Instructions": {
    "push ax, bx": {
      "ExpectedInstructionCount": 9,
      "Comment": [
        "Mergable 16-bit pushes. May or may not be an optimization."
      ],
      "x86Insts": [
        "push ax",
        "push bx"
      ],
      "ExpectedArm64ASM": [
        "mov x20, x4",
        "mov x21, x8",
        "mov x22, x21",
        "strh w20, [x22, #-2]!",
        "mov x8, x22",
        "mov x20, x7",
        "mov x21, x22",
        "strh w20, [x21, #-2]!",
        "mov x8, x21"
      ]
    },
    "push rax, rbx": {
      "ExpectedInstructionCount": 9,
      "Comment": [
        "Mergable 64-bit pushes"
      ],
      "x86Insts": [
        "push rax",
        "push rbx"
      ],
      "ExpectedArm64ASM": [
        "mov x20, x4",
        "mov x21, x8",
        "mov x22, x21",
        "str x20, [x22, #-8]!",
        "mov x8, x22",
        "mov x20, x7",
        "mov x21, x22",
        "str x20, [x21, #-8]!",
        "mov x8, x21"
      ]
    },
    "adds xmm0, xmm1, xmm2": {
      "ExpectedInstructionCount": 11,
      "Comment": [
        "Redundant scalar adds that can get eliminated without AFP."
      ],
      "x86Insts": [
        "addss xmm0, xmm1",
        "addss xmm0, xmm2"
      ],
      "ExpectedArm64ASM": [
        "mov v2.16b, v16.16b",
        "mov v3.16b, v17.16b",
        "mov v4.16b, v2.16b",
        "fadd s0, s2, s3",
        "mov v4.s[0], v0.s[0]",
        "mov v16.16b, v4.16b",
        "mov v2.16b, v18.16b",
        "mov v3.16b, v4.16b",
        "fadd s0, s4, s2",
        "mov v3.s[0], v0.s[0]",
        "mov v16.16b, v3.16b"
      ]
    },
    "positive movsb": {
      "ExpectedInstructionCount": 10,
      "Comment": [
        "When direction flag is a compile time constant we can optimize",
        "loads and stores can turn in to post-increment when known"
      ],
      "x86Insts": [
        "cld",
        "movsb"
      ],
      "ExpectedArm64ASM": [
        "mov w20, #0x1",
        "strb w20, [x28, #714]",
        "mov x20, x10",
        "mov x21, x11",
        "ldrb w22, [x20]",
        "strb w22, [x21]",
        "add x22, x20, #0x1 (1)",
        "add x20, x21, #0x1 (1)",
        "mov x10, x22",
        "mov x11, x20"
      ]
    },
    "positive movsw": {
      "ExpectedInstructionCount": 10,
      "Comment": [
        "When direction flag is a compile time constant we can optimize",
        "loads and stores can turn in to post-increment when known"
      ],
      "x86Insts": [
        "cld",
        "movsw"
      ],
      "ExpectedArm64ASM": [
        "mov w20, #0x1",
        "strb w20, [x28, #714]",
        "mov x20, x10",
        "mov x21, x11",
        "ldrh w22, [x20]",
        "strh w22, [x21]",
        "add x22, x20, #0x2 (2)",
        "add x20, x21, #0x2 (2)",
        "mov x10, x22",
        "mov x11, x20"
      ]
    },
    "positive movsd": {
      "ExpectedInstructionCount": 10,
      "Comment": [
        "When direction flag is a compile time constant we can optimize",
        "loads and stores can turn in to post-increment when known"
      ],
      "x86Insts": [
        "cld",
        "movsd"
      ],
      "ExpectedArm64ASM": [
        "mov w20, #0x1",
        "strb w20, [x28, #714]",
        "mov x20, x10",
        "mov x21, x11",
        "ldr w22, [x20]",
        "str w22, [x21]",
        "add x22, x20, #0x4 (4)",
        "add x20, x21, #0x4 (4)",
        "mov x10, x22",
        "mov x11, x20"
      ]
    },
    "positive movsq": {
      "ExpectedInstructionCount": 10,
      "Comment": [
        "When direction flag is a compile time constant we can optimize",
        "loads and stores can turn in to post-increment when known"
      ],
      "x86Insts": [
        "cld",
        "movsq"
      ],
      "ExpectedArm64ASM": [
        "mov w20, #0x1",
        "strb w20, [x28, #714]",
        "mov x20, x10",
        "mov x21, x11",
        "ldr x22, [x20]",
        "str x22, [x21]",
        "add x22, x20, #0x8 (8)",
        "add x20, x21, #0x8 (8)",
        "mov x10, x22",
        "mov x11, x20"
      ]
    },
    "negative movsb": {
      "ExpectedInstructionCount": 10,
      "Comment": [
        "When direction flag is a compile time constant we can optimize",
        "loads and stores can turn in to post-increment when known"
      ],
      "x86Insts": [
        "std",
        "movsb"
      ],
      "ExpectedArm64ASM": [
        "mov x20, #0xffffffffffffffff",
        "strb w20, [x28, #714]",
        "mov x20, x10",
        "mov x21, x11",
        "ldrb w22, [x20]",
        "strb w22, [x21]",
        "sub x22, x20, #0x1 (1)",
        "sub x20, x21, #0x1 (1)",
        "mov x10, x22",
        "mov x11, x20"
      ]
    },
    "negative movsw": {
      "ExpectedInstructionCount": 10,
      "Comment": [
        "When direction flag is a compile time constant we can optimize",
        "loads and stores can turn in to post-increment when known"
      ],
      "x86Insts": [
        "std",
        "movsw"
      ],
      "ExpectedArm64ASM": [
        "mov x20, #0xffffffffffffffff",
        "strb w20, [x28, #714]",
        "mov x20, x10",
        "mov x21, x11",
        "ldrh w22, [x20]",
        "strh w22, [x21]",
        "sub x22, x20, #0x2 (2)",
        "sub x20, x21, #0x2 (2)",
        "mov x10, x22",
        "mov x11, x20"
      ]
    },
    "negative movsd": {
      "ExpectedInstructionCount": 10,
      "Comment": [
        "When direction flag is a compile time constant we can optimize",
        "loads and stores can turn in to post-increment when known"
      ],
      "x86Insts": [
        "std",
        "movsd"
      ],
      "ExpectedArm64ASM": [
        "mov x20, #0xffffffffffffffff",
        "strb w20, [x28, #714]",
        "mov x20, x10",
        "mov x21, x11",
        "ldr w22, [x20]",
        "str w22, [x21]",
        "sub x22, x20, #0x4 (4)",
        "sub x20, x21, #0x4 (4)",
        "mov x10, x22",
        "mov x11, x20"
      ]
    },
    "negative movsq": {
      "ExpectedInstructionCount": 10,
      "Comment": [
        "When direction flag is a compile time constant we can optimize",
        "loads and stores can turn in to post-increment when known"
      ],
      "x86Insts": [
        "std",
        "movsq"
      ],
      "ExpectedArm64ASM": [
        "mov x20, #0xffffffffffffffff",
        "strb w20, [x28, #714]",
        "mov x20, x10",
        "mov x21, x11",
        "ldr x22, [x20]",
        "str x22, [x21]",
        "sub x22, x20, #0x8 (8)",
        "sub x20, x21, #0x8 (8)",
        "mov x10, x22",
        "mov x11, x20"
      ]
    },
    "positive rep movsb": {
      "ExpectedInstructionCount": 49,
      "Comment": [
        "When direction flag is a compile time constant we can optimize",
        "loads and stores can turn in to post-increment when known"
      ],
      "x86Insts": [
        "cld",
        "rep movsb"
      ],
      "ExpectedArm64ASM": [
        "mov w20, #0x0",
        "mov w21, #0x1",
        "strb w21, [x28, #714]",
        "mov x21, x10",
        "mov x22, x11",
        "mov x23, x5",
        "mov x0, x23",
        "mov x1, x22",
        "mov x2, x21",
        "cbz x0, #+0x78",
        "sub x3, x1, x2",
        "tbz x3, #63, #+0x8",
        "neg x3, x3",
        "sub x3, x3, #0x20 (32)",
        "tbnz x3, #63, #+0x54",
        "sub x0, x0, #0x20 (32)",
        "tbnz x0, #63, #+0x44",
        "sub x0, x0, #0x20 (32)",
        "tbnz x0, #63, #+0x1c",
        "ldp q0, q1, [x2], #32",
        "stp q0, q1, [x1], #32",
        "ldp q0, q1, [x2], #32",
        "stp q0, q1, [x1], #32",
        "sub x0, x0, #0x40 (64)",
        "tbz x0, #63, #-0x14",
        "add x0, x0, #0x40 (64)",
        "cbz x0, #+0x34",
        "sub x0, x0, #0x20 (32)",
        "tbnz x0, #63, #+0x14",
        "ldp q0, q1, [x2], #32",
        "stp q0, q1, [x1], #32",
        "sub x0, x0, #0x20 (32)",
        "tbz x0, #63, #-0xc",
        "add x0, x0, #0x20 (32)",
        "cbz x0, #+0x14",
        "ldrb w3, [x2], #1",
        "strb w3, [x1], #1",
        "sub x0, x0, #0x1 (1)",
        "cbnz x0, #-0xc",
        "mov x0, x22",
        "mov x1, x21",
        "mov x2, x23",
        "add x24, x0, x2",
        "add x25, x1, x2",
        "mov x21, x24",
        "mov x22, x25",
        "mov x5, x20",
        "mov x11, x21",
        "mov x10, x22"
      ]
    },
    "positive rep movsw": {
      "ExpectedInstructionCount": 49,
      "Comment": [
        "When direction flag is a compile time constant we can optimize",
        "loads and stores can turn in to post-increment when known"
      ],
      "x86Insts": [
        "cld",
        "rep movsw"
      ],
      "ExpectedArm64ASM": [
        "mov w20, #0x0",
        "mov w21, #0x1",
        "strb w21, [x28, #714]",
        "mov x21, x10",
        "mov x22, x11",
        "mov x23, x5",
        "mov x0, x23",
        "mov x1, x22",
        "mov x2, x21",
        "cbz x0, #+0x78",
        "sub x3, x1, x2",
        "tbz x3, #63, #+0x8",
        "neg x3, x3",
        "sub x3, x3, #0x20 (32)",
        "tbnz x3, #63, #+0x54",
        "sub x0, x0, #0x10 (16)",
        "tbnz x0, #63, #+0x44",
        "sub x0, x0, #0x10 (16)",
        "tbnz x0, #63, #+0x1c",
        "ldp q0, q1, [x2], #32",
        "stp q0, q1, [x1], #32",
        "ldp q0, q1, [x2], #32",
        "stp q0, q1, [x1], #32",
        "sub x0, x0, #0x20 (32)",
        "tbz x0, #63, #-0x14",
        "add x0, x0, #0x20 (32)",
        "cbz x0, #+0x34",
        "sub x0, x0, #0x10 (16)",
        "tbnz x0, #63, #+0x14",
        "ldp q0, q1, [x2], #32",
        "stp q0, q1, [x1], #32",
        "sub x0, x0, #0x10 (16)",
        "tbz x0, #63, #-0xc",
        "add x0, x0, #0x10 (16)",
        "cbz x0, #+0x14",
        "ldrh w3, [x2], #2",
        "strh w3, [x1], #2",
        "sub x0, x0, #0x1 (1)",
        "cbnz x0, #-0xc",
        "mov x0, x22",
        "mov x1, x21",
        "mov x2, x23",
        "add x24, x0, x2, lsl #1",
        "add x25, x1, x2, lsl #1",
        "mov x21, x24",
        "mov x22, x25",
        "mov x5, x20",
        "mov x11, x21",
        "mov x10, x22"
      ]
    },
    "positive rep movsd": {
      "ExpectedInstructionCount": 49,
      "Comment": [
        "When direction flag is a compile time constant we can optimize",
        "loads and stores can turn in to post-increment when known"
      ],
      "x86Insts": [
        "cld",
        "rep movsd"
      ],
      "ExpectedArm64ASM": [
        "mov w20, #0x0",
        "mov w21, #0x1",
        "strb w21, [x28, #714]",
        "mov x21, x10",
        "mov x22, x11",
        "mov x23, x5",
        "mov x0, x23",
        "mov x1, x22",
        "mov x2, x21",
        "cbz x0, #+0x78",
        "sub x3, x1, x2",
        "tbz x3, #63, #+0x8",
        "neg x3, x3",
        "sub x3, x3, #0x20 (32)",
        "tbnz x3, #63, #+0x54",
        "sub x0, x0, #0x8 (8)",
        "tbnz x0, #63, #+0x44",
        "sub x0, x0, #0x8 (8)",
        "tbnz x0, #63, #+0x1c",
        "ldp q0, q1, [x2], #32",
        "stp q0, q1, [x1], #32",
        "ldp q0, q1, [x2], #32",
        "stp q0, q1, [x1], #32",
        "sub x0, x0, #0x10 (16)",
        "tbz x0, #63, #-0x14",
        "add x0, x0, #0x10 (16)",
        "cbz x0, #+0x34",
        "sub x0, x0, #0x8 (8)",
        "tbnz x0, #63, #+0x14",
        "ldp q0, q1, [x2], #32",
        "stp q0, q1, [x1], #32",
        "sub x0, x0, #0x8 (8)",
        "tbz x0, #63, #-0xc",
        "add x0, x0, #0x8 (8)",
        "cbz x0, #+0x14",
        "ldr w3, [x2], #4",
        "str w3, [x1], #4",
        "sub x0, x0, #0x1 (1)",
        "cbnz x0, #-0xc",
        "mov x0, x22",
        "mov x1, x21",
        "mov x2, x23",
        "add x24, x0, x2, lsl #2",
        "add x25, x1, x2, lsl #2",
        "mov x21, x24",
        "mov x22, x25",
        "mov x5, x20",
        "mov x11, x21",
        "mov x10, x22"
      ]
    },
    "positive rep movsq": {
      "ExpectedInstructionCount": 49,
      "Comment": [
        "When direction flag is a compile time constant we can optimize",
        "loads and stores can turn in to post-increment when known"
      ],
      "x86Insts": [
        "cld",
        "rep movsq"
      ],
      "ExpectedArm64ASM": [
        "mov w20, #0x0",
        "mov w21, #0x1",
        "strb w21, [x28, #714]",
        "mov x21, x10",
        "mov x22, x11",
        "mov x23, x5",
        "mov x0, x23",
        "mov x1, x22",
        "mov x2, x21",
        "cbz x0, #+0x78",
        "sub x3, x1, x2",
        "tbz x3, #63, #+0x8",
        "neg x3, x3",
        "sub x3, x3, #0x20 (32)",
        "tbnz x3, #63, #+0x54",
        "sub x0, x0, #0x4 (4)",
        "tbnz x0, #63, #+0x44",
        "sub x0, x0, #0x4 (4)",
        "tbnz x0, #63, #+0x1c",
        "ldp q0, q1, [x2], #32",
        "stp q0, q1, [x1], #32",
        "ldp q0, q1, [x2], #32",
        "stp q0, q1, [x1], #32",
        "sub x0, x0, #0x8 (8)",
        "tbz x0, #63, #-0x14",
        "add x0, x0, #0x8 (8)",
        "cbz x0, #+0x34",
        "sub x0, x0, #0x4 (4)",
        "tbnz x0, #63, #+0x14",
        "ldp q0, q1, [x2], #32",
        "stp q0, q1, [x1], #32",
        "sub x0, x0, #0x4 (4)",
        "tbz x0, #63, #-0xc",
        "add x0, x0, #0x4 (4)",
        "cbz x0, #+0x14",
        "ldr x3, [x2], #8",
        "str x3, [x1], #8",
        "sub x0, x0, #0x1 (1)",
        "cbnz x0, #-0xc",
        "mov x0, x22",
        "mov x1, x21",
        "mov x2, x23",
        "add x24, x0, x2, lsl #3",
        "add x25, x1, x2, lsl #3",
        "mov x21, x24",
        "mov x22, x25",
        "mov x5, x20",
        "mov x11, x21",
        "mov x10, x22"
      ]
    },
    "negative rep movsb": {
      "ExpectedInstructionCount": 53,
      "Comment": [
        "When direction flag is a compile time constant we can optimize",
        "loads and stores can turn in to post-increment when known"
      ],
      "x86Insts": [
        "std",
        "rep movsb"
      ],
      "ExpectedArm64ASM": [
        "mov x20, #0xffffffffffffffff",
        "strb w20, [x28, #714]",
        "mov x20, x10",
        "mov x21, x11",
        "mov x22, x5",
        "mov x0, x22",
        "mov x1, x21",
        "mov x2, x20",
        "cbz x0, #+0x88",
        "sub x3, x1, x2",
        "tbz x3, #63, #+0x8",
        "neg x3, x3",
        "sub x3, x3, #0x20 (32)",
        "tbnz x3, #63, #+0x64",
        "sub x1, x1, #0x1f (31)",
        "sub x2, x2, #0x1f (31)",
        "sub x0, x0, #0x20 (32)",
        "tbnz x0, #63, #+0x44",
        "sub x0, x0, #0x20 (32)",
        "tbnz x0, #63, #+0x1c",
        "ldp q0, q1, [x2], #-32",
        "stp q0, q1, [x1], #-32",
        "ldp q0, q1, [x2], #-32",
        "stp q0, q1, [x1], #-32",
        "sub x0, x0, #0x40 (64)",
        "tbz x0, #63, #-0x14",
        "add x0, x0, #0x40 (64)",
        "cbz x0, #+0x3c",
        "sub x0, x0, #0x20 (32)",
        "tbnz x0, #63, #+0x14",
        "ldp q0, q1, [x2], #-32",
        "stp q0, q1, [x1], #-32",
        "sub x0, x0, #0x20 (32)",
        "tbz x0, #63, #-0xc",
        "add x0, x0, #0x20 (32)",
        "cbz x0, #+0x1c",
        "add x1, x1, #0x1f (31)",
        "add x2, x2, #0x1f (31)",
        "ldrb w3, [x2], #-1",
        "strb w3, [x1], #-1",
        "sub x0, x0, #0x1 (1)",
        "cbnz x0, #-0xc",
        "mov x0, x21",
        "mov x1, x20",
        "mov x2, x22",
        "sub x24, x0, x2",
        "sub x25, x1, x2",
        "mov x20, x24",
        "mov x21, x25",
        "mov w22, #0x0",
        "mov x5, x22",
        "mov x11, x20",
        "mov x10, x21"
      ]
    },
    "negative rep movsw": {
      "ExpectedInstructionCount": 53,
      "Comment": [
        "When direction flag is a compile time constant we can optimize",
        "loads and stores can turn in to post-increment when known"
      ],
      "x86Insts": [
        "std",
        "rep movsw"
      ],
      "ExpectedArm64ASM": [
        "mov x20, #0xffffffffffffffff",
        "strb w20, [x28, #714]",
        "mov x20, x10",
        "mov x21, x11",
        "mov x22, x5",
        "mov x0, x22",
        "mov x1, x21",
        "mov x2, x20",
        "cbz x0, #+0x88",
        "sub x3, x1, x2",
        "tbz x3, #63, #+0x8",
        "neg x3, x3",
        "sub x3, x3, #0x20 (32)",
        "tbnz x3, #63, #+0x64",
        "sub x1, x1, #0x1e (30)",
        "sub x2, x2, #0x1e (30)",
        "sub x0, x0, #0x10 (16)",
        "tbnz x0, #63, #+0x44",
        "sub x0, x0, #0x10 (16)",
        "tbnz x0, #63, #+0x1c",
        "ldp q0, q1, [x2], #-32",
        "stp q0, q1, [x1], #-32",
        "ldp q0, q1, [x2], #-32",
        "stp q0, q1, [x1], #-32",
        "sub x0, x0, #0x20 (32)",
        "tbz x0, #63, #-0x14",
        "add x0, x0, #0x20 (32)",
        "cbz x0, #+0x3c",
        "sub x0, x0, #0x10 (16)",
        "tbnz x0, #63, #+0x14",
        "ldp q0, q1, [x2], #-32",
        "stp q0, q1, [x1], #-32",
        "sub x0, x0, #0x10 (16)",
        "tbz x0, #63, #-0xc",
        "add x0, x0, #0x10 (16)",
        "cbz x0, #+0x1c",
        "add x1, x1, #0x1e (30)",
        "add x2, x2, #0x1e (30)",
        "ldrh w3, [x2], #-2",
        "strh w3, [x1], #-2",
        "sub x0, x0, #0x1 (1)",
        "cbnz x0, #-0xc",
        "mov x0, x21",
        "mov x1, x20",
        "mov x2, x22",
        "sub x24, x0, x2, lsl #1",
        "sub x25, x1, x2, lsl #1",
        "mov x20, x24",
        "mov x21, x25",
        "mov w22, #0x0",
        "mov x5, x22",
        "mov x11, x20",
        "mov x10, x21"
      ]
    },
    "negative rep movsd": {
      "ExpectedInstructionCount": 53,
      "Comment": [
        "When direction flag is a compile time constant we can optimize",
        "loads and stores can turn in to post-increment when known"
      ],
      "x86Insts": [
        "std",
        "rep movsd"
      ],
      "ExpectedArm64ASM": [
        "mov x20, #0xffffffffffffffff",
        "strb w20, [x28, #714]",
        "mov x20, x10",
        "mov x21, x11",
        "mov x22, x5",
        "mov x0, x22",
        "mov x1, x21",
        "mov x2, x20",
        "cbz x0, #+0x88",
        "sub x3, x1, x2",
        "tbz x3, #63, #+0x8",
        "neg x3, x3",
        "sub x3, x3, #0x20 (32)",
        "tbnz x3, #63, #+0x64",
        "sub x1, x1, #0x1c (28)",
        "sub x2, x2, #0x1c (28)",
        "sub x0, x0, #0x8 (8)",
        "tbnz x0, #63, #+0x44",
        "sub x0, x0, #0x8 (8)",
        "tbnz x0, #63, #+0x1c",
        "ldp q0, q1, [x2], #-32",
        "stp q0, q1, [x1], #-32",
        "ldp q0, q1, [x2], #-32",
        "stp q0, q1, [x1], #-32",
        "sub x0, x0, #0x10 (16)",
        "tbz x0, #63, #-0x14",
        "add x0, x0, #0x10 (16)",
        "cbz x0, #+0x3c",
        "sub x0, x0, #0x8 (8)",
        "tbnz x0, #63, #+0x14",
        "ldp q0, q1, [x2], #-32",
        "stp q0, q1, [x1], #-32",
        "sub x0, x0, #0x8 (8)",
        "tbz x0, #63, #-0xc",
        "add x0, x0, #0x8 (8)",
        "cbz x0, #+0x1c",
        "add x1, x1, #0x1c (28)",
        "add x2, x2, #0x1c (28)",
        "ldr w3, [x2], #-4",
        "str w3, [x1], #-4",
        "sub x0, x0, #0x1 (1)",
        "cbnz x0, #-0xc",
        "mov x0, x21",
        "mov x1, x20",
        "mov x2, x22",
        "sub x24, x0, x2, lsl #2",
        "sub x25, x1, x2, lsl #2",
        "mov x20, x24",
        "mov x21, x25",
        "mov w22, #0x0",
        "mov x5, x22",
        "mov x11, x20",
        "mov x10, x21"
      ]
    },
    "negative rep movsq": {
      "ExpectedInstructionCount": 53,
      "Comment": [
        "When direction flag is a compile time constant we can optimize",
        "loads and stores can turn in to post-increment when known"
      ],
      "x86Insts": [
        "std",
        "rep movsq"
      ],
      "ExpectedArm64ASM": [
        "mov x20, #0xffffffffffffffff",
        "strb w20, [x28, #714]",
        "mov x20, x10",
        "mov x21, x11",
        "mov x22, x5",
        "mov x0, x22",
        "mov x1, x21",
        "mov x2, x20",
        "cbz x0, #+0x88",
        "sub x3, x1, x2",
        "tbz x3, #63, #+0x8",
        "neg x3, x3",
        "sub x3, x3, #0x20 (32)",
        "tbnz x3, #63, #+0x64",
        "sub x1, x1, #0x18 (24)",
        "sub x2, x2, #0x18 (24)",
        "sub x0, x0, #0x4 (4)",
        "tbnz x0, #63, #+0x44",
        "sub x0, x0, #0x4 (4)",
        "tbnz x0, #63, #+0x1c",
        "ldp q0, q1, [x2], #-32",
        "stp q0, q1, [x1], #-32",
        "ldp q0, q1, [x2], #-32",
        "stp q0, q1, [x1], #-32",
        "sub x0, x0, #0x8 (8)",
        "tbz x0, #63, #-0x14",
        "add x0, x0, #0x8 (8)",
        "cbz x0, #+0x3c",
        "sub x0, x0, #0x4 (4)",
        "tbnz x0, #63, #+0x14",
        "ldp q0, q1, [x2], #-32",
        "stp q0, q1, [x1], #-32",
        "sub x0, x0, #0x4 (4)",
        "tbz x0, #63, #-0xc",
        "add x0, x0, #0x4 (4)",
        "cbz x0, #+0x1c",
        "add x1, x1, #0x18 (24)",
        "add x2, x2, #0x18 (24)",
        "ldr x3, [x2], #-8",
        "str x3, [x1], #-8",
        "sub x0, x0, #0x1 (1)",
        "cbnz x0, #-0xc",
        "mov x0, x21",
        "mov x1, x20",
        "mov x2, x22",
        "sub x24, x0, x2, lsl #3",
        "sub x25, x1, x2, lsl #3",
        "mov x20, x24",
        "mov x21, x25",
        "mov w22, #0x0",
        "mov x5, x22",
        "mov x11, x20",
        "mov x10, x21"
      ]
    },
    "positive rep stosb": {
      "ExpectedInstructionCount": 34,
      "Comment": [
        "When direction flag is a compile time constant we can optimize",
        "loads and stores can turn in to post-increment when known"
      ],
      "x86Insts": [
        "cld",
        "rep stosb"
      ],
      "ExpectedArm64ASM": [
        "mov w20, #0x0",
        "mov w21, #0x1",
        "strb w21, [x28, #714]",
        "mov x21, x4",
        "uxtb w22, w21",
        "mov x21, x11",
        "mov x23, x5",
        "mov x0, x23",
        "mov x1, x21",
        "cbz x0, #+0x58",
        "sub x0, x0, #0x20 (32)",
        "tbnz x0, #63, #+0x3c",
        "dup v1.16b, w22",
        "sub x0, x0, #0x20 (32)",
        "tbnz x0, #63, #+0x14",
        "stp q1, q1, [x1], #32",
        "stp q1, q1, [x1], #32",
        "sub x0, x0, #0x40 (64)",
        "tbz x0, #63, #-0xc",
        "add x0, x0, #0x40 (64)",
        "cbz x0, #+0x2c",
        "sub x0, x0, #0x20 (32)",
        "tbnz x0, #63, #+0x10",
        "stp q1, q1, [x1], #32",
        "sub x0, x0, #0x20 (32)",
        "tbz x0, #63, #-0x8",
        "add x0, x0, #0x20 (32)",
        "cbz x0, #+0x10",
        "strb w22, [x1], #1",
        "sub x0, x0, #0x1 (1)",
        "cbnz x0, #-0x8",
        "add x24, x21, x23",
        "mov x5, x20",
        "mov x11, x24"
      ]
    },
    "positive rep stosw": {
      "ExpectedInstructionCount": 34,
      "Comment": [
        "When direction flag is a compile time constant we can optimize",
        "loads and stores can turn in to post-increment when known"
      ],
      "x86Insts": [
        "cld",
        "rep stosw"
      ],
      "ExpectedArm64ASM": [
        "mov w20, #0x0",
        "mov w21, #0x1",
        "strb w21, [x28, #714]",
        "mov x21, x4",
        "uxth w22, w21",
        "mov x21, x11",
        "mov x23, x5",
        "mov x0, x23",
        "mov x1, x21",
        "cbz x0, #+0x58",
        "sub x0, x0, #0x10 (16)",
        "tbnz x0, #63, #+0x3c",
        "dup v1.8h, w22",
        "sub x0, x0, #0x10 (16)",
        "tbnz x0, #63, #+0x14",
        "stp q1, q1, [x1], #32",
        "stp q1, q1, [x1], #32",
        "sub x0, x0, #0x20 (32)",
        "tbz x0, #63, #-0xc",
        "add x0, x0, #0x20 (32)",
        "cbz x0, #+0x2c",
        "sub x0, x0, #0x10 (16)",
        "tbnz x0, #63, #+0x10",
        "stp q1, q1, [x1], #32",
        "sub x0, x0, #0x10 (16)",
        "tbz x0, #63, #-0x8",
        "add x0, x0, #0x10 (16)",
        "cbz x0, #+0x10",
        "strh w22, [x1], #2",
        "sub x0, x0, #0x1 (1)",
        "cbnz x0, #-0x8",
        "add x24, x21, x23, lsl #1",
        "mov x5, x20",
        "mov x11, x24"
      ]
    },
    "positive rep stosd": {
      "ExpectedInstructionCount": 34,
      "Comment": [
        "When direction flag is a compile time constant we can optimize",
        "loads and stores can turn in to post-increment when known"
      ],
      "x86Insts": [
        "cld",
        "rep stosd"
      ],
      "ExpectedArm64ASM": [
        "mov w20, #0x0",
        "mov w21, #0x1",
        "strb w21, [x28, #714]",
        "mov x21, x4",
        "mov w22, w21",
        "mov x21, x11",
        "mov x23, x5",
        "mov x0, x23",
        "mov x1, x21",
        "cbz x0, #+0x58",
        "sub x0, x0, #0x8 (8)",
        "tbnz x0, #63, #+0x3c",
        "dup v1.4s, w22",
        "sub x0, x0, #0x8 (8)",
        "tbnz x0, #63, #+0x14",
        "stp q1, q1, [x1], #32",
        "stp q1, q1, [x1], #32",
        "sub x0, x0, #0x10 (16)",
        "tbz x0, #63, #-0xc",
        "add x0, x0, #0x10 (16)",
        "cbz x0, #+0x2c",
        "sub x0, x0, #0x8 (8)",
        "tbnz x0, #63, #+0x10",
        "stp q1, q1, [x1], #32",
        "sub x0, x0, #0x8 (8)",
        "tbz x0, #63, #-0x8",
        "add x0, x0, #0x8 (8)",
        "cbz x0, #+0x10",
        "str w22, [x1], #4",
        "sub x0, x0, #0x1 (1)",
        "cbnz x0, #-0x8",
        "add x24, x21, x23, lsl #2",
        "mov x5, x20",
        "mov x11, x24"
      ]
    },
    "positive rep stosq": {
      "ExpectedInstructionCount": 33,
      "Comment": [
        "When direction flag is a compile time constant we can optimize",
        "loads and stores can turn in to post-increment when known"
      ],
      "x86Insts": [
        "cld",
        "rep stosq"
      ],
      "ExpectedArm64ASM": [
        "mov w20, #0x0",
        "mov w21, #0x1",
        "strb w21, [x28, #714]",
        "mov x21, x4",
        "mov x22, x11",
        "mov x23, x5",
        "mov x0, x23",
        "mov x1, x22",
        "cbz x0, #+0x58",
        "sub x0, x0, #0x4 (4)",
        "tbnz x0, #63, #+0x3c",
        "dup v1.2d, x21",
        "sub x0, x0, #0x4 (4)",
        "tbnz x0, #63, #+0x14",
        "stp q1, q1, [x1], #32",
        "stp q1, q1, [x1], #32",
        "sub x0, x0, #0x8 (8)",
        "tbz x0, #63, #-0xc",
        "add x0, x0, #0x8 (8)",
        "cbz x0, #+0x2c",
        "sub x0, x0, #0x4 (4)",
        "tbnz x0, #63, #+0x10",
        "stp q1, q1, [x1], #32",
        "sub x0, x0, #0x4 (4)",
        "tbz x0, #63, #-0x8",
        "add x0, x0, #0x4 (4)",
        "cbz x0, #+0x10",
        "str x21, [x1], #8",
        "sub x0, x0, #0x1 (1)",
        "cbnz x0, #-0x8",
        "add x24, x22, x23, lsl #3",
        "mov x5, x20",
        "mov x11, x24"
      ]
    },
    "negative rep stosb": {
      "ExpectedInstructionCount": 36,
      "Comment": [
        "When direction flag is a compile time constant we can optimize",
        "loads and stores can turn in to post-increment when known"
      ],
      "x86Insts": [
        "std",
        "rep stosb"
      ],
      "ExpectedArm64ASM": [
        "mov x20, #0xffffffffffffffff",
        "strb w20, [x28, #714]",
        "mov x20, x4",
        "uxtb w21, w20",
        "mov x20, x11",
        "mov x22, x5",
        "mov x0, x22",
        "mov x1, x20",
        "cbz x0, #+0x60",
        "sub x1, x1, #0x1f (31)",
        "sub x0, x0, #0x20 (32)",
        "tbnz x0, #63, #+0x3c",
        "dup v1.16b, w21",
        "sub x0, x0, #0x20 (32)",
        "tbnz x0, #63, #+0x14",
        "stp q1, q1, [x1], #-32",
        "stp q1, q1, [x1], #-32",
        "sub x0, x0, #0x40 (64)",
        "tbz x0, #63, #-0xc",
        "add x0, x0, #0x40 (64)",
        "cbz x0, #+0x30",
        "sub x0, x0, #0x20 (32)",
        "tbnz x0, #63, #+0x10",
        "stp q1, q1, [x1], #-32",
        "sub x0, x0, #0x20 (32)",
        "tbz x0, #63, #-0x8",
        "add x0, x0, #0x20 (32)",
        "cbz x0, #+0x14",
        "add x1, x1, #0x1f (31)",
        "strb w21, [x1], #-1",
        "sub x0, x0, #0x1 (1)",
        "cbnz x0, #-0x8",
        "sub x23, x20, x22",
        "mov w20, #0x0",
        "mov x5, x20",
        "mov x11, x23"
      ]
    },
    "negative rep stosw": {
      "ExpectedInstructionCount": 36,
      "Comment": [
        "When direction flag is a compile time constant we can optimize",
        "loads and stores can turn in to post-increment when known"
      ],
      "x86Insts": [
        "std",
        "rep stosw"
      ],
      "ExpectedArm64ASM": [
        "mov x20, #0xffffffffffffffff",
        "strb w20, [x28, #714]",
        "mov x20, x4",
        "uxth w21, w20",
        "mov x20, x11",
        "mov x22, x5",
        "mov x0, x22",
        "mov x1, x20",
        "cbz x0, #+0x60",
        "sub x1, x1, #0x1e (30)",
        "sub x0, x0, #0x10 (16)",
        "tbnz x0, #63, #+0x3c",
        "dup v1.8h, w21",
        "sub x0, x0, #0x10 (16)",
        "tbnz x0, #63, #+0x14",
        "stp q1, q1, [x1], #-32",
        "stp q1, q1, [x1], #-32",
        "sub x0, x0, #0x20 (32)",
        "tbz x0, #63, #-0xc",
        "add x0, x0, #0x20 (32)",
        "cbz x0, #+0x30",
        "sub x0, x0, #0x10 (16)",
        "tbnz x0, #63, #+0x10",
        "stp q1, q1, [x1], #-32",
        "sub x0, x0, #0x10 (16)",
        "tbz x0, #63, #-0x8",
        "add x0, x0, #0x10 (16)",
        "cbz x0, #+0x14",
        "add x1, x1, #0x1e (30)",
        "strh w21, [x1], #-2",
        "sub x0, x0, #0x1 (1)",
        "cbnz x0, #-0x8",
        "sub x23, x20, x22, lsl #1",
        "mov w20, #0x0",
        "mov x5, x20",
        "mov x11, x23"
      ]
    },
    "negative rep stosd": {
      "ExpectedInstructionCount": 36,
      "Comment": [
        "When direction flag is a compile time constant we can optimize",
        "loads and stores can turn in to post-increment when known"
      ],
      "x86Insts": [
        "std",
        "rep stosd"
      ],
      "ExpectedArm64ASM": [
        "mov x20, #0xffffffffffffffff",
        "strb w20, [x28, #714]",
        "mov x20, x4",
        "mov w21, w20",
        "mov x20, x11",
        "mov x22, x5",
        "mov x0, x22",
        "mov x1, x20",
        "cbz x0, #+0x60",
        "sub x1, x1, #0x1c (28)",
        "sub x0, x0, #0x8 (8)",
        "tbnz x0, #63, #+0x3c",
        "dup v1.4s, w21",
        "sub x0, x0, #0x8 (8)",
        "tbnz x0, #63, #+0x14",
        "stp q1, q1, [x1], #-32",
        "stp q1, q1, [x1], #-32",
        "sub x0, x0, #0x10 (16)",
        "tbz x0, #63, #-0xc",
        "add x0, x0, #0x10 (16)",
        "cbz x0, #+0x30",
        "sub x0, x0, #0x8 (8)",
        "tbnz x0, #63, #+0x10",
        "stp q1, q1, [x1], #-32",
        "sub x0, x0, #0x8 (8)",
        "tbz x0, #63, #-0x8",
        "add x0, x0, #0x8 (8)",
        "cbz x0, #+0x14",
        "add x1, x1, #0x1c (28)",
        "str w21, [x1], #-4",
        "sub x0, x0, #0x1 (1)",
        "cbnz x0, #-0x8",
        "sub x23, x20, x22, lsl #2",
        "mov w20, #0x0",
        "mov x5, x20",
        "mov x11, x23"
      ]
    },
    "negative rep stosq": {
      "ExpectedInstructionCount": 35,
      "Comment": [
        "When direction flag is a compile time constant we can optimize",
        "loads and stores can turn in to post-increment when known"
      ],
      "x86Insts": [
        "std",
        "rep stosq"
      ],
      "ExpectedArm64ASM": [
        "mov x20, #0xffffffffffffffff",
        "strb w20, [x28, #714]",
        "mov x20, x4",
        "mov x21, x11",
        "mov x22, x5",
        "mov x0, x22",
        "mov x1, x21",
        "cbz x0, #+0x60",
        "sub x1, x1, #0x18 (24)",
        "sub x0, x0, #0x4 (4)",
        "tbnz x0, #63, #+0x3c",
        "dup v1.2d, x20",
        "sub x0, x0, #0x4 (4)",
        "tbnz x0, #63, #+0x14",
        "stp q1, q1, [x1], #-32",
        "stp q1, q1, [x1], #-32",
        "sub x0, x0, #0x8 (8)",
        "tbz x0, #63, #-0xc",
        "add x0, x0, #0x8 (8)",
        "cbz x0, #+0x30",
        "sub x0, x0, #0x4 (4)",
        "tbnz x0, #63, #+0x10",
        "stp q1, q1, [x1], #-32",
        "sub x0, x0, #0x4 (4)",
        "tbz x0, #63, #-0x8",
        "add x0, x0, #0x4 (4)",
        "cbz x0, #+0x14",
        "add x1, x1, #0x18 (24)",
        "str x20, [x1], #-8",
        "sub x0, x0, #0x1 (1)",
        "cbnz x0, #-0x8",
        "sub x23, x21, x22, lsl #3",
        "mov w20, #0x0",
        "mov x5, x20",
        "mov x11, x23"
      ]
    },
    "Sekiro spill block": {
      "ExpectedInstructionCount": 329,
      "Comment": [
        "This block of code came from the settings screen when it loaded",
        "It was originally at RIP: 0x14232cca0 and has been deobfuscated"
      ],
      "x86Insts": [
        "mov    QWORD [rsp+0x8],rcx",
        "push   rbx",
        "push   rbp",
        "push   rsi",
        "push   rdi",
        "push   r12",
        "push   r13",
        "push   r14",
        "push   r15",
        "sub    rsp,0x18",
        "mov    ecx,dword [rdx+0x24]",
        "mov    esi,dword [rdx]",
        "mov    ebp,dword [rdx+0x4]",
        "mov    r14d,dword [rdx+0x8]",
        "mov    r15d,dword [rdx+0xc]",
        "mov    r12d,dword [rdx+0x10]",
        "mov    r13d,dword [rdx+0x14]",
        "mov    r11d,dword [rdx+0x18]",
        "mov    ebx,dword [rdx+0x1c]",
        "mov    edi,dword [rdx+0x20]",
        "imul   eax,ecx,0x13",
        "mov    dword [rsp+0x68],ecx",
        "add    eax,0x1000000",
        "shr    eax,0x19",
        "add    eax,esi",
        "sar    eax,0x1a",
        "add    eax,ebp",
        "sar    eax,0x19",
        "add    eax,r14d",
        "sar    eax,0x1a",
        "add    eax,r15d",
        "sar    eax,0x19",
        "add    eax,r12d",
        "sar    eax,0x1a",
        "add    eax,r13d",
        "sar    eax,0x19",
        "add    eax,r11d",
        "sar    eax,0x1a",
        "add    eax,ebx",
        "sar    eax,0x19",
        "add    eax,edi",
        "sar    eax,0x1a",
        "add    eax,ecx",
        "sar    eax,0x19",
        "imul   eax,eax,0x13",
        "add    esi,eax",
        "mov    eax,esi",
        "sar    eax,0x1a",
        "add    ebp,eax",
        "shl    eax,0x1a",
        "sub    esi,eax",
        "mov    ecx,ebp",
        "mov    rax,qword [rsp+0x60]",
        "sar    ecx,0x19",
        "add    r14d,ecx",
        "shl    ecx,0x19",
        "mov    edx,r14d",
        "sub    ebp,ecx",
        "sar    edx,0x1a",
        "add    r15d,edx",
        "mov    dword [rax],esi",
        "mov    r8d,r15d",
        "shl    edx,0x1a",
        "sar    r8d,0x19",
        "sub    r14d,edx",
        "add    r12d,r8d",
        "mov    dword [rax+0x4],ebp",
        "mov    r9d,r12d",
        "shl    r8d,0x19",
        "sar    r9d,0x1a",
        "sub    r15d,r8d",
        "add    r13d,r9d",
        "mov    dword [rax+0x8],r14d",
        "shl    r9d,0x1a",
        "mov    r10d,r13d",
        "sar    r10d,0x19",
        "sub    r12d,r9d",
        "add    r11d,r10d",
        "mov    dword [rax+0xc],r15d",
        "mov    dword [rsp+0x70],r11d",
        "mov    rsi,rax",
        "sar    r11d,0x1a",
        "add    ebx,r11d",
        "mov    dword [rax+0x10],r12d",
        "mov    dword [rsp+0x78],ebx",
        "sar    ebx,0x19",
        "add    edi,ebx",
        "mov    dword [rsp],edi",
        "sar    edi,0x1a",
        "add    dword [rsp+0x68],edi",
        "shl    r10d,0x19",
        "mov    ecx,dword [rsp+0x68]",
        "sub    r13d,r10d",
        "mov    dword [rax+0x14],r13d",
        "mov    eax,dword [rsp+0x70]",
        "shl    r11d,0x1a",
        "sub    eax,r11d",
        "shl    ebx,0x19",
        "mov    dword [rsi+0x18],eax",
        "mov    eax,dword [rsp+0x78]",
        "sub    eax,ebx",
        "shl    edi,0x1a",
        "mov    dword [rsi+0x1c],eax",
        "mov    eax,dword [rsp]",
        "sub    eax,edi",
        "mov    dword [rsi+0x20],eax",
        "mov    eax,ecx",
        "and    eax,0xfe000000",
        "sub    ecx,eax",
        "mov    dword [rsi+0x24],ecx",
        "add    rsp,0x18",
        "pop    r15",
        "pop    r14",
        "pop    r13",
        "pop    r12",
        "pop    rdi",
        "pop    rsi",
        "pop    rbp",
        "pop    rbx"
      ],
      "ExpectedArm64ASM": [
        "sub sp, sp, #0x160 (352)",
        "mov x20, x5",
        "mov x21, x8",
        "str x20, [x21, #8]",
        "mov x20, x7",
        "mov x22, x21",
        "str x20, [x22, #-8]!",
        "mov x8, x22",
        "mov x20, x9",
        "mov x21, x22",
        "str x20, [x21, #-8]!",
        "mov x8, x21",
        "mov x20, x10",
        "mov x22, x21",
        "str x20, [x22, #-8]!",
        "mov x8, x22",
        "mov x20, x11",
        "mov x21, x22",
        "str x20, [x21, #-8]!",
        "mov x8, x21",
        "mov x20, x16",
        "mov x22, x21",
        "str x20, [x22, #-8]!",
        "mov x8, x22",
        "mov x20, x17",
        "mov x21, x22",
        "str x20, [x21, #-8]!",
        "mov x8, x21",
        "mov x20, x19",
        "mov x22, x21",
        "str x20, [x22, #-8]!",
        "mov x8, x22",
        "mov x20, x29",
        "mov x21, x22",
        "str x20, [x21, #-8]!",
        "mov x8, x21",
        "sub x20, x21, #0x18 (24)",
        "mov x8, x20",
        "mov x21, x6",
        "ldr w22, [x21, #36]",
        "mov x5, x22",
        "ldr w23, [x21]",
        "mov x10, x23",
        "ldr w24, [x21, #4]",
        "mov x9, x24",
        "ldr w25, [x21, #8]",
        "mov x19, x25",
        "ldr w30, [x21, #12]",
        "mov x29, x30",
        "str w30, [sp]",
        "ldr w30, [x21, #16]",
        "mov x16, x30",
        "str w30, [sp, #32]",
        "ldr w30, [x21, #20]",
        "mov x17, x30",
        "str w30, [sp, #64]",
        "ldr w30, [x21, #24]",
        "mov x15, x30",
        "str w30, [sp, #96]",
        "ldr w30, [x21, #28]",
        "mov x7, x30",
        "str w30, [sp, #128]",
        "ldr w30, [x21, #32]",
        "mov x11, x30",
        "mov w21, #0x13",
        "str w30, [sp, #160]",
        "mul w30, w22, w21",
        "mov x4, x30",
        "str w22, [x20, #104]",
        "str x20, [sp, #192]",
        "mov w20, #0x1000000",
        "str w21, [sp, #224]",
        "add w21, w30, w20",
        "mov x4, x21",
        "mov w20, w21",
        "lsr w21, w20, #25",
        "mov x4, x21",
        "add w20, w21, w23",
        "mov x4, x20",
        "mov w21, w20",
        "asr w20, w21, #26",
        "mov x4, x20",
        "add w21, w20, w24",
        "mov x4, x21",
        "mov w20, w21",
        "asr w21, w20, #25",
        "mov x4, x21",
        "add w20, w21, w25",
        "mov x4, x20",
        "mov w21, w20",
        "asr w20, w21, #26",
        "mov x4, x20",
        "ldr w21, [sp]",
        "add w30, w20, w21",
        "mov x4, x30",
        "mov w20, w30",
        "asr w30, w20, #25",
        "mov x4, x30",
        "ldr w20, [sp, #32]",
        "add w21, w30, w20",
        "mov x4, x21",
        "mov w30, w21",
        "asr w21, w30, #26",
        "mov x4, x21",
        "ldr w30, [sp, #64]",
        "add w20, w21, w30",
        "mov x4, x20",
        "mov w21, w20",
        "asr w20, w21, #25",
        "mov x4, x20",
        "ldr w21, [sp, #96]",
        "add w30, w20, w21",
        "mov x4, x30",
        "mov w20, w30",
        "asr w30, w20, #26",
        "mov x4, x30",
        "ldr w20, [sp, #128]",
        "add w21, w30, w20",
        "mov x4, x21",
        "mov w30, w21",
        "asr w21, w30, #25",
        "mov x4, x21",
        "ldr w30, [sp, #160]",
        "add w20, w21, w30",
        "mov x4, x20",
        "mov w21, w20",
        "asr w20, w21, #26",
        "mov x4, x20",
        "add w21, w20, w22",
        "mov x4, x21",
        "mov w20, w21",
        "asr w21, w20, #25",
        "mov x4, x21",
        "ldr w20, [sp, #224]",
        "mul w22, w21, w20",
        "mov x4, x22",
        "add w20, w23, w22",
        "mov x10, x20",
        "mov w21, w20",
        "mov x4, x21",
        "asr w22, w21, #26",
        "mov x4, x22",
        "add w21, w24, w22",
        "mov x9, x21",
        "mov w23, w22",
        "lsl w22, w23, #26",
        "mov x4, x22",
        "sub w23, w20, w22",
        "mov x10, x23",
        "mov w20, w21",
        "mov x5, x20",
        "ldr x22, [sp, #192]",
        "ldr x24, [x22, #96]",
        "mov x4, x24",
        "asr w30, w20, #25",
        "mov x5, x30",
        "add w20, w25, w30",
        "mov x19, x20",
        "mov w25, w30",
        "lsl w30, w25, #25",
        "mov x5, x30",
        "mov w25, w20",
        "mov x6, x25",
        "sub w22, w21, w30",
        "mov x9, x22",
        "asr w21, w25, #26",
        "mov x6, x21",
        "ldr w25, [sp]",
        "add w30, w25, w21",
        "mov x29, x30",
        "mov w25, w23",
        "str w25, [x24]",
        "mov w23, w30",
        "mov x12, x23",
        "mov w25, w21",
        "lsl w21, w25, #26",
        "mov x6, x21",
        "asr w25, w23, #25",
        "mov x12, x25",
        "sub w23, w20, w21",
        "mov x19, x23",
        "ldr w20, [sp, #32]",
        "add w21, w20, w25",
        "mov x16, x21",
        "mov w20, w22",
        "str w20, [x24, #4]",
        "mov w20, w21",
        "mov x13, x20",
        "mov w22, w25",
        "lsl w25, w22, #25",
        "mov x12, x25",
        "asr w22, w20, #26",
        "mov x13, x22",
        "sub w20, w30, w25",
        "mov x29, x20",
        "ldr w25, [sp, #64]",
        "add w30, w25, w22",
        "mov x17, x30",
        "mov w25, w23",
        "str w25, [x24, #8]",
        "mov w23, w22",
        "lsl w22, w23, #26",
        "mov x13, x22",
        "mov w23, w30",
        "mov x14, x23",
        "asr w25, w23, #25",
        "mov x14, x25",
        "sub w23, w21, w22",
        "mov x16, x23",
        "ldr w21, [sp, #96]",
        "add w22, w21, w25",
        "mov x15, x22",
        "mov w21, w20",
        "str w21, [x24, #12]",
        "mov w20, w22",
        "ldr x21, [sp, #192]",
        "str w20, [x21, #112]",
        "mov x10, x24",
        "mov w20, w22",
        "asr w22, w20, #26",
        "mov x15, x22",
        "ldr w20, [sp, #128]",
        "str w30, [sp, #256]",
        "add w30, w20, w22",
        "mov x7, x30",
        "mov w20, w23",
        "str w20, [x24, #16]",
        "mov w20, w30",
        "str w20, [x21, #120]",
        "mov w20, w30",
        "asr w23, w20, #25",
        "mov x7, x23",
        "ldr w20, [sp, #160]",
        "add w30, w20, w23",
        "mov x11, x30",
        "mov w20, w30",
        "str w20, [x21]",
        "mov w20, w30",
        "asr w30, w20, #26",
        "mov x11, x30",
        "ldr w20, [x21, #104]",
        "str w23, [sp, #288]",
        "add w23, w20, w30",
        "str w23, [x21, #104]",
        "mov w20, w25",
        "lsl w23, w20, #25",
        "mov x14, x23",
        "ldr w20, [x21, #104]",
        "mov x5, x20",
        "ldr w25, [sp, #256]",
        "str w20, [sp, #320]",
        "sub w20, w25, w23",
        "mov x17, x20",
        "mov w23, w20",
        "str w23, [x24, #20]",
        "ldr w20, [x21, #112]",
        "mov x4, x20",
        "mov w23, w22",
        "lsl w22, w23, #26",
        "mov x15, x22",
        "sub w23, w20, w22",
        "mov x4, x23",
        "ldr w20, [sp, #288]",
        "mov w22, w20",
        "lsl w20, w22, #25",
        "mov x7, x20",
        "mov w22, w23",
        "str w22, [x24, #24]",
        "ldr w22, [x21, #120]",
        "mov x4, x22",
        "sub w23, w22, w20",
        "mov x4, x23",
        "mov w20, w30",
        "lsl w22, w20, #26",
        "mov x11, x22",
        "mov w20, w23",
        "str w20, [x24, #28]",
        "ldr w20, [x21]",
        "mov x4, x20",
        "sub w23, w20, w22",
        "mov x4, x23",
        "mov w20, w23",
        "str w20, [x24, #32]",
        "ldr w20, [sp, #320]",
        "mov x4, x20",
        "and w22, w20, #0xfe000000",
        "mov x4, x22",
        "sub w23, w20, w22",
        "mov x5, x23",
        "mov w20, w23",
        "str w20, [x24, #36]",
        "mvn w20, w21",
        "mov x27, x20",
        "adds x20, x21, #0x18 (24)",
        "mov x26, x20",
        "mov x8, x20",
        "ldr x21, [x20]",
        "add x22, x20, #0x8 (8)",
        "mov x8, x22",
        "mov x29, x21",
        "ldr x21, [x20, #8]",
        "add x20, x22, #0x8 (8)",
        "mov x8, x20",
        "mov x19, x21",
        "ldr x21, [x22, #8]",
        "add x22, x20, #0x8 (8)",
        "mov x8, x22",
        "mov x17, x21",
        "ldr x21, [x20, #8]",
        "add x20, x22, #0x8 (8)",
        "mov x8, x20",
        "mov x16, x21",
        "ldr x21, [x22, #8]",
        "add x22, x20, #0x8 (8)",
        "mov x8, x22",
        "mov x11, x21",
        "ldr x21, [x20, #8]",
        "add x20, x22, #0x8 (8)",
        "mov x8, x20",
        "mov x10, x21",
        "ldr x21, [x22, #8]",
        "add x22, x20, #0x8 (8)",
        "mov x8, x22",
        "mov x9, x21",
        "ldr x21, [x20, #8]",
        "add x20, x22, #0x8 (8)",
        "mov x8, x20",
        "mov x7, x21",
        "add sp, sp, #0x160 (352)"
      ]
    }
  }
}
